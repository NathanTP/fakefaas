#XXX This is pseudocode for now to help me think
import kaas
import cnn

#==============================================================================
# This is setup that would really be out-of-band. In theory this stuff would
# already be in the data layer.
#==============================================================================
imgs, lbls = cnn.loadMnist()

kaas.kv.put("imgs", imgs)
kaas.kv.put("lbls", lbls)

# KV will have a bunch of "l#_params" entries for the serialized layer
# descriptions (dimX, dimY, dimOut, weights, biases)
layers = []
for i in range(3):
    layers[i] = cnn.layerParamsFromFile(modelPath / ('l' + str(i))
    kaas.kv.put("layer" + str(i) + "_params", cnn.serialize(layers[i]))

# These are some representation of device-only code (maybe a library with thin
# wrappers, or some IR or just raw CUDA code). Our first pass will be a
# pre-compiled shared library with thin wrappers around the kernels.
#
# I have these being registered in the offline setup phase in this example.
# They can be dynamic if you want as well. You could imagine something like
# pytorch or onnxruntime compiling these and uploading them to kaas to start
# executing, much like they register the kernels with CUDA now.
kaas.register(cnn.layer0Forward)
kaas.register(cnn.layer1Forward)
kaas.register(cnn.layer2Forward)

#==============================================================================
# A hypothetical serving frontend would run something like the following.
#
# This is pretty similar to cloudburst, but with some key differences:
#   * The functions being put into the execution DAG are not arbitrary python
#     functions, they are accelerator kernels.
#   * All memories are declared up-front. Sizes are all explicit. Data sources
#     are explicit.
#   * The kernels may be quite fine-grained, but they are described in such a
#     way that they can be fused easily. Depending on how far we push the PL side
#     of things, our system could even fuse them into the same binary or kernels.
#   * There are no implicit code dependencies, no packages, no python versions,
#     nothing. The only dependencies are the code itself and the input/output
#     buffers.
#
# That being said, a chain of kernel invocations could be just a node in a
# normal cloudburst graph, and the two would be co-optimized (e.g. the kernel
# would preferentially run where the inputs are hot in the Anna cache or where
# the next function is going to run).
#==============================================================================

# A req is a description of the kernel chain to call. Eventually it could be a
# DAG but for now it's a simple chain of calls. While users could write these,
# it's also reasonable to expect them to be packaged up so they look like
# regular functions (maybe generated by DL frameworks).
req = [
    {
        # Inputs are loaded into device memory from the KV store, a pointer to them
        # will be handed to the kernel. Here we pass all the imgs, but we'd
        # probably break them into separate keys per batch, or specify a subset of
        # a shared array abstraction
        inputs=["imgs", "layer0_params"],

        # Kernels cannot call cudaMalloc, they have to pre-declare any memories
        # they need. This buffer will be provided to the kernel and filled with
        # zeros. It will not be saved after, even between kernels in the same
        # request (if you want a temporary buffer that's passed within a
        # request, use non-persisted outputs).
        tmps=[kaas.buffer("layer1_act", layers[0].actSize, persist=False)],

        # Like tmps, outputs are allocated by KaaS, but they will be saved to the
        # KV after the kernel finishes. They can be the same key as an input, in
        # which case no new buffer will be allocated, instead the kernel intends to
        # mutate the input buffer and save it somewhere else after. The persist
        # flag tells the system whether the object should be persisted beyond the
        # current execution graph. Persisting may or may not be slow (it can happen
        # in the background).
        outputs=[kaas.buffer("layer0_output", layers[0].outSize, persist=False)]
    },
    {

        # layer0_output was not persisted, but it is visible within the same
        # req. Since the system has all the information, layer1_act might get
        # freed since you can't reference it anymore anyway.
        inputs=["layer0_output", "layer1_params"],
        tmps=[kaas.buffer("act", layers[1].actSize, persist=False)],
        outputs=[kaas.buffer("layer1_output", layers[1].outSize, persist=False)]
    },
    {
        inputs=["layer1_output", "layer2_params"],
        tmps=[kaas.buffer("act", layers[2].actSize, persist=False)],

        # Only the output of the last layer should be persisted beyond the session
        outputs=[kaas.buffer("layer2_output", layers[2].outSize, persist=True)]
    }
]

kaas.invoke(req)
rawPreds = kaas.kv.get("layer2_output")
preds = np.frombuffer(rawPred, dtype=np.float32)

bestPred = np.argmax(preds)
print(bestPred)
