//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30300941
// Cuda compilation tools, release 11.4, V11.4.120
// Based on NVVM 7.0.1
//

.version 7.4
.target sm_35
.address_size 64

	// .globl	_Z9atomicAddPdd
// _ZZ12JacobiMethodE8b_shared has been demoted
.extern .shared .align 8 .b8 x_shared[];

.visible .func  (.param .b64 func_retval0) _Z9atomicAddPdd(
	.param .b64 _Z9atomicAddPdd_param_0,
	.param .b64 _Z9atomicAddPdd_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd4, [_Z9atomicAddPdd_param_0];
	ld.param.f64 	%fd2, [_Z9atomicAddPdd_param_1];
	ld.u64 	%rd6, [%rd4];

$L__BB0_1:
	mov.b64 	%fd1, %rd6;
	add.f64 	%fd3, %fd1, %fd2;
	mov.b64 	%rd5, %fd3;
	atom.cas.b64 	%rd3, [%rd4], %rd6, %rd5;
	setp.ne.s64 	%p1, %rd6, %rd3;
	mov.u64 	%rd6, %rd3;
	@%p1 bra 	$L__BB0_1;

	st.param.f64 	[func_retval0+0], %fd1;
	ret;

}
	// .globl	JacobiMethod
.visible .entry JacobiMethod(
	.param .u32 JacobiMethod_param_0,
	.param .u64 JacobiMethod_param_1,
	.param .u64 JacobiMethod_param_2,
	.param .u64 JacobiMethod_param_3,
	.param .u64 JacobiMethod_param_4,
	.param .u64 JacobiMethod_param_5
)
{
	.reg .pred 	%p<39>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<140>;
	.reg .f64 	%fd<47>;
	.reg .b64 	%rd<33>;
	// demoted variable
	.shared .align 8 .b8 _ZZ12JacobiMethodE8b_shared[72];

	ld.param.u32 	%r26, [JacobiMethod_param_0];
	ld.param.u64 	%rd16, [JacobiMethod_param_1];
	ld.param.u64 	%rd12, [JacobiMethod_param_2];
	ld.param.u64 	%rd13, [JacobiMethod_param_3];
	ld.param.u64 	%rd14, [JacobiMethod_param_4];
	ld.param.u64 	%rd15, [JacobiMethod_param_5];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd17, %rd15;
	mov.u64 	%rd18, 0;
	st.global.u64 	[%rd17], %rd18;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r26;
	@%p1 bra 	$L__BB1_3;

	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r28, x_shared;
	mov.u32 	%r132, %r1;

$L__BB1_2:
	mul.wide.s32 	%rd19, %r132, 8;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.f64 	%fd9, [%rd20];
	shl.b32 	%r27, %r132, 3;
	add.s32 	%r29, %r28, %r27;
	st.shared.f64 	[%r29], %fd9;
	add.s32 	%r132, %r132, %r2;
	setp.lt.s32 	%p2, %r132, %r26;
	@%p2 bra 	$L__BB1_2;

$L__BB1_3:
	setp.gt.u32 	%p3, %r1, 7;
	mov.u32 	%r30, %ctaid.x;
	shl.b32 	%r5, %r30, 3;
	@%p3 bra 	$L__BB1_7;

	add.s32 	%r133, %r5, %r1;
	setp.ge.s32 	%p4, %r133, %r26;
	@%p4 bra 	$L__BB1_7;

	cvta.to.global.u64 	%rd3, %rd12;
	mov.u32 	%r134, %r1;

$L__BB1_6:
	mul.wide.s32 	%rd21, %r133, 8;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.f64 	%fd10, [%rd22];
	mul.hi.s32 	%r31, %r133, 954437177;
	shr.u32 	%r32, %r31, 31;
	shr.u32 	%r33, %r31, 1;
	add.s32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, 9;
	sub.s32 	%r36, %r133, %r35;
	shl.b32 	%r37, %r36, 3;
	mov.u32 	%r38, _ZZ12JacobiMethodE8b_shared;
	add.s32 	%r39, %r38, %r37;
	st.shared.f64 	[%r39], %fd10;
	add.s32 	%r9, %r134, 8;
	add.s32 	%r133, %r133, 8;
	setp.lt.s32 	%p5, %r133, %r26;
	setp.lt.s32 	%p6, %r134, 0;
	and.pred  	%p7, %p6, %p5;
	mov.u32 	%r134, %r9;
	@%p7 bra 	$L__BB1_6;

$L__BB1_7:
	barrier.sync 	0;
	setp.ge.s32 	%p8, %r5, %r26;
	@%p8 bra 	$L__BB1_16;

	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r42, %tid.z;
	mov.u32 	%r43, %tid.y;
	mad.lo.s32 	%r44, %r41, %r42, %r43;
	mad.lo.s32 	%r45, %r44, %r11, %r1;
	and.b32  	%r12, %r45, 31;
	mov.u32 	%r136, 0;
	mov.f64 	%fd11, 0d0000000000000000;
	setp.ne.s32 	%p21, %r12, 0;
	mov.u32 	%r135, %r5;

$L__BB1_9:
	mov.f64 	%fd44, %fd11;
	@%p1 bra 	$L__BB1_12;

	mul.lo.s32 	%r15, %r135, %r26;
	mov.u32 	%r137, %r1;
	mov.f64 	%fd44, %fd11;

$L__BB1_11:
	add.s32 	%r46, %r137, %r15;
	mul.wide.s32 	%rd23, %r46, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f1, [%rd24];
	cvt.f64.f32 	%fd13, %f1;
	shl.b32 	%r47, %r137, 3;
	mov.u32 	%r48, x_shared;
	add.s32 	%r49, %r48, %r47;
	ld.shared.f64 	%fd14, [%r49];
	fma.rn.f64 	%fd44, %fd14, %fd13, %fd44;
	add.s32 	%r137, %r137, %r11;
	setp.lt.s32 	%p10, %r137, %r26;
	@%p10 bra 	$L__BB1_11;

$L__BB1_12:
	// begin inline asm
	mov.b64 {%r50,%r51}, %fd44;
	// end inline asm
	mov.u32 	%r70, 2;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r72, %r71, %r73;
	shfl.sync.down.b32 	%r52|%p12, %r50, %r72, %r71, %r73;
	// begin inline asm
	mov.b64 %fd16, {%r52,%r53};
	// end inline asm
	add.f64 	%fd17, %fd44, %fd16;
	// begin inline asm
	mov.b64 {%r54,%r55}, %fd17;
	// end inline asm
	mov.u32 	%r74, 8;
	shfl.sync.down.b32 	%r57|%p13, %r55, %r74, %r71, %r73;
	shfl.sync.down.b32 	%r56|%p14, %r54, %r74, %r71, %r73;
	// begin inline asm
	mov.b64 %fd18, {%r56,%r57};
	// end inline asm
	add.f64 	%fd19, %fd17, %fd18;
	// begin inline asm
	mov.b64 {%r58,%r59}, %fd19;
	// end inline asm
	mov.u32 	%r75, 4;
	shfl.sync.down.b32 	%r61|%p15, %r59, %r75, %r71, %r73;
	shfl.sync.down.b32 	%r60|%p16, %r58, %r75, %r71, %r73;
	// begin inline asm
	mov.b64 %fd20, {%r60,%r61};
	// end inline asm
	add.f64 	%fd21, %fd19, %fd20;
	// begin inline asm
	mov.b64 {%r62,%r63}, %fd21;
	// end inline asm
	shfl.sync.down.b32 	%r65|%p17, %r63, %r70, %r71, %r73;
	shfl.sync.down.b32 	%r64|%p18, %r62, %r70, %r71, %r73;
	// begin inline asm
	mov.b64 %fd22, {%r64,%r65};
	// end inline asm
	add.f64 	%fd23, %fd21, %fd22;
	// begin inline asm
	mov.b64 {%r66,%r67}, %fd23;
	// end inline asm
	mov.u32 	%r76, 1;
	shfl.sync.down.b32 	%r69|%p19, %r67, %r76, %r71, %r73;
	shfl.sync.down.b32 	%r68|%p20, %r66, %r76, %r71, %r73;
	// begin inline asm
	mov.b64 %fd24, {%r68,%r69};
	// end inline asm
	add.f64 	%fd4, %fd23, %fd24;
	@%p21 bra 	$L__BB1_15;

	mul.hi.s32 	%r77, %r135, 954437177;
	shr.u32 	%r78, %r77, 31;
	shr.u32 	%r79, %r77, 1;
	add.s32 	%r80, %r79, %r78;
	mul.lo.s32 	%r81, %r80, 9;
	sub.s32 	%r82, %r135, %r81;
	shl.b32 	%r83, %r82, 3;
	mov.u32 	%r84, _ZZ12JacobiMethodE8b_shared;
	add.s32 	%r18, %r84, %r83;
	ld.shared.u64 	%rd31, [%r18];

$L__BB1_14:
	mov.b64 	%fd25, %rd31;
	sub.f64 	%fd26, %fd25, %fd4;
	mov.b64 	%rd25, %fd26;
	atom.shared.cas.b64 	%rd6, [%r18], %rd31, %rd25;
	setp.ne.s64 	%p22, %rd31, %rd6;
	mov.u64 	%rd31, %rd6;
	@%p22 bra 	$L__BB1_14;

$L__BB1_15:
	add.s32 	%r136, %r136, 1;
	setp.lt.u32 	%p23, %r136, 8;
	add.s32 	%r135, %r135, 1;
	setp.lt.s32 	%p24, %r135, %r26;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	$L__BB1_9;

$L__BB1_16:
	barrier.sync 	0;
	@%p3 bra 	$L__BB1_23;

	// begin inline asm
	mov.u32 %r85, %laneid;
	// end inline asm
	add.s32 	%r138, %r5, %r1;
	setp.ge.s32 	%p27, %r138, %r26;
	mov.f64 	%fd46, 0d0000000000000000;
	@%p27 bra 	$L__BB1_20;

	cvta.to.global.u64 	%rd7, %rd14;
	mov.f64 	%fd46, 0d0000000000000000;
	mov.u32 	%r139, %r1;

$L__BB1_19:
	mul.hi.s32 	%r86, %r138, 954437177;
	shr.u32 	%r87, %r86, 31;
	shr.u32 	%r88, %r86, 1;
	add.s32 	%r89, %r88, %r87;
	mul.lo.s32 	%r90, %r89, 9;
	sub.s32 	%r91, %r138, %r90;
	shl.b32 	%r92, %r91, 3;
	mov.u32 	%r93, _ZZ12JacobiMethodE8b_shared;
	add.s32 	%r94, %r93, %r92;
	mad.lo.s32 	%r95, %r138, %r26, %r138;
	mul.wide.s32 	%rd26, %r95, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.f32 	%f2, [%rd27];
	cvt.f64.f32 	%fd29, %f2;
	ld.shared.f64 	%fd30, [%r94];
	div.rn.f64 	%fd31, %fd30, %fd29;
	shl.b32 	%r96, %r138, 3;
	mov.u32 	%r97, x_shared;
	add.s32 	%r98, %r97, %r96;
	ld.shared.f64 	%fd32, [%r98];
	add.f64 	%fd33, %fd32, %fd31;
	mul.wide.s32 	%rd28, %r138, 8;
	add.s64 	%rd29, %rd7, %rd28;
	st.global.f64 	[%rd29], %fd33;
	abs.f64 	%fd34, %fd31;
	add.f64 	%fd46, %fd46, %fd34;
	add.s32 	%r24, %r139, 8;
	add.s32 	%r138, %r138, 8;
	setp.lt.s32 	%p28, %r138, %r26;
	setp.lt.s32 	%p29, %r139, 0;
	and.pred  	%p30, %p29, %p28;
	mov.u32 	%r139, %r24;
	@%p30 bra 	$L__BB1_19;

$L__BB1_20:
	// begin inline asm
	mov.u32 %r99, %laneid;
	// end inline asm
	and.b32  	%r114, %r99, -8;
	mov.u32 	%r115, 255;
	shl.b32 	%r116, %r115, %r114;
	// begin inline asm
	mov.b64 {%r100,%r101}, %fd46;
	// end inline asm
	mov.u32 	%r117, 2;
	mov.u32 	%r118, 6175;
	mov.u32 	%r119, 4;
	shfl.sync.down.b32 	%r103|%p31, %r101, %r119, %r118, %r116;
	shfl.sync.down.b32 	%r102|%p32, %r100, %r119, %r118, %r116;
	// begin inline asm
	mov.b64 %fd36, {%r102,%r103};
	// end inline asm
	add.f64 	%fd37, %fd46, %fd36;
	// begin inline asm
	mov.u32 %r104, %laneid;
	// end inline asm
	and.b32  	%r120, %r104, -8;
	shl.b32 	%r121, %r115, %r120;
	// begin inline asm
	mov.b64 {%r105,%r106}, %fd37;
	// end inline asm
	shfl.sync.down.b32 	%r108|%p33, %r106, %r117, %r118, %r121;
	shfl.sync.down.b32 	%r107|%p34, %r105, %r117, %r118, %r121;
	// begin inline asm
	mov.b64 %fd38, {%r107,%r108};
	// end inline asm
	add.f64 	%fd39, %fd37, %fd38;
	// begin inline asm
	mov.u32 %r109, %laneid;
	// end inline asm
	and.b32  	%r122, %r109, -8;
	shl.b32 	%r123, %r115, %r122;
	// begin inline asm
	mov.b64 {%r110,%r111}, %fd39;
	// end inline asm
	mov.u32 	%r124, 1;
	shfl.sync.down.b32 	%r113|%p35, %r111, %r124, %r118, %r123;
	shfl.sync.down.b32 	%r112|%p36, %r110, %r124, %r118, %r123;
	// begin inline asm
	mov.b64 %fd40, {%r112,%r113};
	// end inline asm
	add.f64 	%fd8, %fd39, %fd40;
	mov.u32 	%r125, %ntid.y;
	mov.u32 	%r126, %tid.z;
	mov.u32 	%r127, %tid.y;
	mad.lo.s32 	%r128, %r125, %r126, %r127;
	mov.u32 	%r129, %ntid.x;
	mad.lo.s32 	%r130, %r128, %r129, %r1;
	and.b32  	%r131, %r130, 7;
	setp.ne.s32 	%p37, %r131, 0;
	@%p37 bra 	$L__BB1_23;

	ld.global.u64 	%rd32, [%rd17];

$L__BB1_22:
	mov.b64 	%fd41, %rd32;
	add.f64 	%fd42, %fd8, %fd41;
	mov.b64 	%rd30, %fd42;
	atom.global.cas.b64 	%rd11, [%rd17], %rd32, %rd30;
	setp.ne.s64 	%p38, %rd32, %rd11;
	mov.u64 	%rd32, %rd11;
	@%p38 bra 	$L__BB1_22;

$L__BB1_23:
	ret;

}

